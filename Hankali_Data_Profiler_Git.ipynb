{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6aac7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-08 03:08:59.743 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.pyplot as plt\n",
    "from openai.error import RateLimitError\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = '****'  # Replace with your OpenAI API key\n",
    "\n",
    "# CSS\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .main {\n",
    "        background-color: #f0f2f6;\n",
    "        padding: 20px;\n",
    "    }\n",
    "    .stButton button {\n",
    "        background-color: #1f77b4;\n",
    "        color: white;\n",
    "        font-size: 16px;\n",
    "        border-radius: 8px;\n",
    "        padding: 10px;\n",
    "    }\n",
    "    .stTextArea textarea {\n",
    "        background-color: #e3f2fd;\n",
    "        border-radius: 8px;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Helper function to determine the type of each column\n",
    "def data_profile(df):\n",
    "    profile = {}\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            unique_values = df[col].nunique()\n",
    "            if unique_values > 10:\n",
    "                profile[col] = \"Continuous\"\n",
    "            else:\n",
    "                profile[col] = \"Discrete\"\n",
    "        elif pd.api.types.is_categorical_dtype(df[col]) or df[col].dtype == 'object':\n",
    "            profile[col] = \"Categorical\"\n",
    "        else:\n",
    "            profile[col] = \"Other\"\n",
    "    return profile\n",
    "\n",
    "# Missing values check\n",
    "def missing_values(df):\n",
    "    return df.isnull().sum()\n",
    "\n",
    "# Unique values check\n",
    "def unique_values(df):\n",
    "    return df.nunique()\n",
    "\n",
    "# Descriptive statistics\n",
    "def summary_stats(df):\n",
    "    return df.describe()\n",
    "\n",
    "# Cardinality check for categorical columns\n",
    "def cardinality(df):\n",
    "    return df.select_dtypes(include=['object']).nunique()\n",
    "\n",
    "# Correlation matrix (Fix: Only select numerical columns)\n",
    "def correlation_matrix(df):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])  \n",
    "    if numeric_df.empty:\n",
    "        return \"No numerical columns available for correlation.\"\n",
    "    return numeric_df.corr()\n",
    "\n",
    "# Outlier detection using IQR (Fix: Only numeric columns)\n",
    "def detect_outliers(df):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])  \n",
    "    if numeric_df.empty:\n",
    "        return \"No numerical columns available for outlier detection.\"\n",
    "    \n",
    "    Q1 = numeric_df.quantile(0.25)\n",
    "    Q3 = numeric_df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    return (numeric_df < (Q1 - 1.5 * IQR)) | (numeric_df > (Q3 + 1.5 * IQR))\n",
    "\n",
    "# Duplicate rows check\n",
    "def duplicate_rows(df):\n",
    "    return df.duplicated().sum()\n",
    "\n",
    "# Memory usage check\n",
    "def memory_usage(df):\n",
    "    return df.memory_usage(deep=True)\n",
    "\n",
    "# Zero and constant values check\n",
    "def zero_constant_check(df):\n",
    "    zero_columns = (df == 0).sum()\n",
    "    constant_columns = df.nunique() == 1\n",
    "    return zero_columns, constant_columns\n",
    "\n",
    "# Time series decomposition (for time-series data)\n",
    "def time_series_analysis(df, column):\n",
    "    df[column] = pd.to_datetime(df[column])\n",
    "    decomposition = seasonal_decompose(df[column], model='additive')\n",
    "    return decomposition\n",
    "\n",
    "# Histogram visualization for continuous variables\n",
    "def plot_histograms(df):\n",
    "    numeric_df = df.select_dtypes(include=[np.number]) \n",
    "    continuous_columns = [col for col in numeric_df.columns if numeric_df[col].nunique() > 10]  \n",
    "    if not continuous_columns:\n",
    "        st.write(\"No continuous variables available for histogram.\")\n",
    "    else:\n",
    "        for col in continuous_columns:\n",
    "            st.subheader(f\"Histogram for {col}\")\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.hist(numeric_df[col].dropna(), bins=20, color='blue', edgecolor='black')\n",
    "            ax.set_title(f\"Distribution of {col}\")\n",
    "            ax.set_xlabel(col)\n",
    "            ax.set_ylabel(\"Frequency\")\n",
    "            st.pyplot(fig)\n",
    "\n",
    "# Function to send data and description to OpenAI's API\n",
    "def get_openai_response(dataset, description):\n",
    "    # Limit the dataset to a reasonable size\n",
    "    dataset_sample = dataset.iloc[:50, :10]  # Limit to 50 rows and 10 columns\n",
    "\n",
    "    # Prepare dataset as a JSON string\n",
    "    dataset_json = dataset_sample.to_json()\n",
    "\n",
    "    # Prepare the message for OpenAI GPT model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a data analysis assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Dataset: {dataset_json}\\n\\nDescription: {description}\"}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Send the API request\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"****\", # Replace with the model you intend to use\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.choices[0].message['content']\n",
    "\n",
    "    except RateLimitError:\n",
    "        # Display a friendly message if the data is too large\n",
    "        return \"The dataset is too large to process. Please reduce the size or sample the data.\"\n",
    "\n",
    "# Streamlit app interface\n",
    "st.title(\"üîç Comprehensive Data Profiler\")\n",
    "st.sidebar.title(\"üîß Settings & Upload\")\n",
    "st.sidebar.markdown(\"Upload your dataset and choose from various profiling options.\")\n",
    "\n",
    "# File upload option\n",
    "uploaded_file = st.sidebar.file_uploader(\"Choose an Excel, CSV, or JSON file\", type=['xlsx', 'csv', 'json'])\n",
    "\n",
    "# Textbox for brief description\n",
    "dataset_brief = st.sidebar.text_area(\"üí° Enter a brief description of the dataset:\")\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Read dataset based on file format\n",
    "    if uploaded_file.name.endswith('.xlsx'):\n",
    "        df = pd.read_excel(uploaded_file)  # Ensure openpyxl is installed\n",
    "    elif uploaded_file.name.endswith('.csv'):\n",
    "        df = pd.read_csv(uploaded_file)\n",
    "    elif uploaded_file.name.endswith('.json'):\n",
    "        df = pd.read_json(uploaded_file)\n",
    "\n",
    "    # Display first 20 columns of the dataset\n",
    "    st.subheader(\"üî¢ First 20 Columns of the Dataset\")\n",
    "    df_subset = df.iloc[:, :20]\n",
    "    st.dataframe(df_subset)\n",
    "\n",
    "    # Profile data types and categorize\n",
    "    with st.expander(\"üìä Data Profile\"):\n",
    "        st.write(\"The data type for each column:\")\n",
    "        profile = data_profile(df_subset)\n",
    "        profile_df = pd.DataFrame(list(profile.items()), columns=[\"Column\", \"Type\"])\n",
    "        st.dataframe(profile_df)\n",
    "\n",
    "    # Missing values check\n",
    "    with st.expander(\"‚ùì Missing Values\"):\n",
    "        missing_vals = missing_values(df_subset)\n",
    "        st.dataframe(missing_vals)\n",
    "\n",
    "    # Unique values check\n",
    "    with st.expander(\"üîç Unique Values\"):\n",
    "        unique_vals = unique_values(df_subset)\n",
    "        st.dataframe(unique_vals)\n",
    "\n",
    "    # Summary statistics\n",
    "    with st.expander(\"üìà Summary Statistics\"):\n",
    "        summary = summary_stats(df_subset)\n",
    "        st.dataframe(summary)\n",
    "\n",
    "    # Cardinality check for categorical columns\n",
    "    with st.expander(\"üî¢ Cardinality of Categorical Columns\"):\n",
    "        cardinality_vals = cardinality(df_subset)\n",
    "        st.dataframe(cardinality_vals)\n",
    "\n",
    "    # Correlation matrix (Numerical columns only)\n",
    "    with st.expander(\"üìä Correlation Matrix\"):\n",
    "        corr_matrix = correlation_matrix(df_subset)\n",
    "        if isinstance(corr_matrix, str):\n",
    "            st.write(corr_matrix)  # No numeric columns\n",
    "        else:\n",
    "            st.dataframe(corr_matrix)\n",
    "\n",
    "    # Outlier detection (Numerical columns only)\n",
    "    with st.expander(\"üîç Outlier Detection\"):\n",
    "        outliers = detect_outliers(df_subset)\n",
    "        if isinstance(outliers, str):\n",
    "            st.write(outliers)  # No numeric columns\n",
    "        else:\n",
    "            st.dataframe(outliers)\n",
    "\n",
    "    # Histogram for continuous variables\n",
    "    with st.expander(\"üìä Histogram for Continuous Variables\"):\n",
    "        plot_histograms(df_subset)\n",
    "\n",
    "    # Duplicate rows check\n",
    "    with st.expander(\"‚ôªÔ∏è Duplicate Rows Check\"):\n",
    "        duplicates = duplicate_rows(df_subset)\n",
    "        st.write(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "    # Memory usage check\n",
    "    with st.expander(\"üíæ Memory Usage\"):\n",
    "        memory_usage_vals = memory_usage(df_subset)\n",
    "        st.dataframe(memory_usage_vals)\n",
    "\n",
    "    # Zero and constant values check\n",
    "    with st.expander(\"üî¢ Zero and Constant Values Check\"):\n",
    "        zero_vals, constant_vals = zero_constant_check(df_subset)\n",
    "        st.write(\"Zero Value Counts per Column:\")\n",
    "        st.dataframe(zero_vals)\n",
    "        st.write(\"Constant Value Columns (True means constant):\")\n",
    "        st.dataframe(constant_vals)\n",
    "\n",
    "    # Time series analysis (if applicable)\n",
    "    with st.expander(\"‚è≥ Time Series Analysis\"):\n",
    "        datetime_columns = df_subset.select_dtypes(include=['datetime64', 'datetime'])\n",
    "        if not datetime_columns.empty:\n",
    "            column_to_analyze = st.selectbox(\"Select date-time column to analyze:\", datetime_columns.columns)\n",
    "            if column_to_analyze:\n",
    "                decomposition = time_series_analysis(df_subset, column_to_analyze)\n",
    "                decomposition.plot()\n",
    "                st.pyplot()\n",
    "        else:\n",
    "            st.write(\"No Date/Time columns detected for time series analysis.\")\n",
    "\n",
    "    # Button to send a smaller dataset and description to ChatGPT API\n",
    "    if st.sidebar.button(\"Send Data\"):\n",
    "        # Check if description is provided\n",
    "        if not dataset_brief:\n",
    "            st.error(\"Please provide a brief description of the dataset.\")\n",
    "        else:\n",
    "            # Call the OpenAI API function with limited rows and columns\n",
    "            with st.spinner(\"Sending a smaller subset of data to ChatGPT...\"):\n",
    "                response = get_openai_response(df_subset, dataset_brief)\n",
    "            \n",
    "            # Display the response\n",
    "            st.subheader(\"AI Powered Bot Response\")\n",
    "            st.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc145fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
